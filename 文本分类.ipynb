{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "文本分类.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb4Po99ro01n",
        "colab_type": "code",
        "outputId": "723fa7a7-a1d9-4561-ae51-1330fefcb54d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "#切分文本,得到数据集\n",
        "def loadDataSet(file):\n",
        "    df = pd.read_table(file)\n",
        "    textList = df.Phrase.map(lambda x: x.lower()).str.split('\\W+')\n",
        "    classVec= df['Sentiment']\n",
        "    return textList,classVec\n",
        "  \n",
        "textList, classVec = loadDataSet('train.tsv')\n",
        "print(textList,classVec)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     textList, classVec = loadDataSet('train.tsv')\n",
        "#     vocabList = createVocabList(textList)\n",
        "#     returnVec = SetOfWords2Vec(vocabList, textList)\n",
        "#     print(returnVec)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0         [a, series, of, escapades, demonstrating, the,...\n",
            "1         [a, series, of, escapades, demonstrating, the,...\n",
            "2                                               [a, series]\n",
            "3                                                       [a]\n",
            "4                                                  [series]\n",
            "5         [of, escapades, demonstrating, the, adage, tha...\n",
            "6                                                      [of]\n",
            "7         [escapades, demonstrating, the, adage, that, w...\n",
            "8                                               [escapades]\n",
            "9         [demonstrating, the, adage, that, what, is, go...\n",
            "10                              [demonstrating, the, adage]\n",
            "11                                          [demonstrating]\n",
            "12                                             [the, adage]\n",
            "13                                                    [the]\n",
            "14                                                  [adage]\n",
            "15                  [that, what, is, good, for, the, goose]\n",
            "16                                                   [that]\n",
            "17                        [what, is, good, for, the, goose]\n",
            "18                                                   [what]\n",
            "19                              [is, good, for, the, goose]\n",
            "20                                                     [is]\n",
            "21                                  [good, for, the, goose]\n",
            "22                                                   [good]\n",
            "23                                        [for, the, goose]\n",
            "24                                                    [for]\n",
            "25                                             [the, goose]\n",
            "26                                                  [goose]\n",
            "27        [is, also, good, for, the, gander, some, of, w...\n",
            "28        [is, also, good, for, the, gander, some, of, w...\n",
            "29                                               [is, also]\n",
            "                                ...                        \n",
            "156030                   [a, joke, in, the, united, states]\n",
            "156031    [the, movie, s, downfall, is, to, substitute, ...\n",
            "156032                            [the, movie, s, downfall]\n",
            "156033       [is, to, substitute, plot, for, personality, ]\n",
            "156034         [is, to, substitute, plot, for, personality]\n",
            "156035             [to, substitute, plot, for, personality]\n",
            "156036                 [substitute, plot, for, personality]\n",
            "156037                                   [substitute, plot]\n",
            "156038                                   [for, personality]\n",
            "156039    [the, film, is, darkly, atmospheric, with, her...\n",
            "156040    [is, darkly, atmospheric, with, herrmann, quie...\n",
            "156041    [is, darkly, atmospheric, with, herrmann, quie...\n",
            "156042                          [is, darkly, atmospheric, ]\n",
            "156043                            [is, darkly, atmospheric]\n",
            "156044    [with, herrmann, quietly, suggesting, the, sad...\n",
            "156045    [herrmann, quietly, suggesting, the, sadness, ...\n",
            "156046                                           [herrmann]\n",
            "156047    [quietly, suggesting, the, sadness, and, obses...\n",
            "156048    [suggesting, the, sadness, and, obsession, ben...\n",
            "156049           [suggesting, the, sadness, and, obsession]\n",
            "156050                       [the, sadness, and, obsession]\n",
            "156051                            [sadness, and, obsession]\n",
            "156052                                       [sadness, and]\n",
            "156053    [beneath, hearst, s, forced, avuncular, chortles]\n",
            "156054             [hearst, s, forced, avuncular, chortles]\n",
            "156055                                          [hearst, s]\n",
            "156056                        [forced, avuncular, chortles]\n",
            "156057                                [avuncular, chortles]\n",
            "156058                                          [avuncular]\n",
            "156059                                           [chortles]\n",
            "Name: Phrase, Length: 156060, dtype: object 0         1\n",
            "1         2\n",
            "2         2\n",
            "3         2\n",
            "4         2\n",
            "5         2\n",
            "6         2\n",
            "7         2\n",
            "8         2\n",
            "9         2\n",
            "10        2\n",
            "11        2\n",
            "12        2\n",
            "13        2\n",
            "14        2\n",
            "15        2\n",
            "16        2\n",
            "17        2\n",
            "18        2\n",
            "19        2\n",
            "20        2\n",
            "21        3\n",
            "22        3\n",
            "23        2\n",
            "24        2\n",
            "25        2\n",
            "26        2\n",
            "27        2\n",
            "28        2\n",
            "29        2\n",
            "         ..\n",
            "156030    2\n",
            "156031    1\n",
            "156032    1\n",
            "156033    1\n",
            "156034    1\n",
            "156035    2\n",
            "156036    1\n",
            "156037    2\n",
            "156038    2\n",
            "156039    2\n",
            "156040    2\n",
            "156041    2\n",
            "156042    2\n",
            "156043    3\n",
            "156044    2\n",
            "156045    2\n",
            "156046    2\n",
            "156047    1\n",
            "156048    2\n",
            "156049    2\n",
            "156050    2\n",
            "156051    1\n",
            "156052    1\n",
            "156053    2\n",
            "156054    2\n",
            "156055    2\n",
            "156056    1\n",
            "156057    3\n",
            "156058    2\n",
            "156059    2\n",
            "Name: Sentiment, Length: 156060, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAYVNO2iY5y5",
        "colab_type": "code",
        "outputId": "31de316b-818d-4d73-907e-9139f5155b5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "# 根据词频选择部分词汇作为vocabulary\n",
        "def createVocabList(textList):\n",
        "#     vocabSet0 = set([])\n",
        "    vocabListSet = {}\n",
        "    for document in textList:\n",
        "#         vocabSet0 = vocabSet0 | set(document)\n",
        "        for word in document:\n",
        "            if word not in vocabListSet.keys():\n",
        "                vocabListSet[word] = 1\n",
        "            else:\n",
        "                vocabListSet[word] += 1  \n",
        "    sorted_vocabListSet = sorted(vocabListSet.items(),key=lambda x:x[1],reverse=True)\n",
        "    stwlist = [line.strip() for line in open('stopwords.txt', encoding='utf-8').readlines()]\n",
        "    sorted_vocabList = []\n",
        "    for i in range(len(sorted_vocabListSet)):\n",
        "        sorted_vocabList.append(sorted_vocabListSet[i][0])\n",
        "#     return sorted_vocabList\n",
        "\n",
        "    vocabList = []\n",
        "    for word in sorted_vocabList:\n",
        "        if word not in stwlist and vocabListSet[word] >800 and len(word)>1:\n",
        "            vocabList.append(word)\n",
        "#     return sorted_vocabListSet\n",
        "    return vocabList\n",
        "\n",
        "vocabList = createVocabList(textList)\n",
        "print(vocabList)\n",
        "print(len(vocabList))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['film', 'movie', 'story', 'rrb', 'good', 'lrb', 'time', 'characters', 'comedy', 'life', 'funny', 'make', 'movies', 'director', 'love', 'bad', 'work', 'made', 'action', 'people', 'makes', 'character', 'world', 'plot', 'films', 'drama', 'audience', 'long', 'big', 'sense', 'man', 'feel', 'real', 'great', 'humor', 'screen', 'year', 'minutes']\n",
            "38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X0huv59i4Nv",
        "colab_type": "code",
        "outputId": "2637c608-f746-4ef5-e0b0-905c6eda1aa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "# one-hot编码\n",
        "def SetOfWords2Vec(vocabList,textList):\n",
        "    returnVec = []\n",
        "    for i in  range(len(textList)):                \n",
        "        returnVeci = [0]*len(vocabList)\n",
        "        for word in textList.iloc[i]:\n",
        "            if word in vocabList:\n",
        "                returnVeci[vocabList.index(word)] = 1\n",
        "        returnVec.append(returnVeci)\n",
        "    return returnVec\n",
        "\n",
        "returnVec = SetOfWords2Vec(vocabList,textList)\n",
        "# print(returnVec[0:10])\n",
        "print(np.array(returnVec).shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(156060, 38)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wolykx0Uxcfa",
        "colab_type": "code",
        "outputId": "35f60a3c-9f97-46ca-92b9-89f694127c77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#定义softmax函数\n",
        "class Softmax(object):\n",
        "    def __init__(self):\n",
        "        self.learning_rate = 0.0001           # 学习速率\n",
        "        self.max_iteration = 10000            # 最大迭代次数\n",
        "        self.weight_lambda = 0.01             # 衰退权重\n",
        "\n",
        "    def cal_e(self,x,l):\n",
        "        '''\n",
        "        softmax分子\n",
        "        '''\n",
        "        theta_l = self.w[l]\n",
        "        product = np.dot(theta_l,x)\n",
        "        return math.exp(product)\n",
        "\n",
        "    def cal_probability(self,x,j):\n",
        "        '''\n",
        "        softmax\n",
        "        '''\n",
        "        molecule = self.cal_e(x,j)\n",
        "        denominator = sum([self.cal_e(x,i) for i in range(self.k)])\n",
        "        return molecule/denominator\n",
        "\n",
        "    def cal_partial_derivative(self,x,y,j):\n",
        "        '''\n",
        "        计算theta j 的梯度\n",
        "        '''\n",
        "        first = int(y==j)                           # 计算示性函数\n",
        "        second = self.cal_probability(x,j)          # 计算后面那个概率\n",
        "        return -x*(first-second) + self.weight_lambda*self.w[j]\n",
        "       \n",
        "    def train(self, features, labels):\n",
        "        self.k = len(set(labels))\n",
        "        self.w = np.zeros((self.k,len(features[0])+1))  # w的维度：k*特征个数+1\n",
        "        time = 0\n",
        "        while time < self.max_iteration:\n",
        "#             print('loop %d' % time)\n",
        "            time += 1\n",
        "            index = random.randint(0, len(labels) - 1) #产生一个随机数\n",
        "            x = features[index]\n",
        "            y = labels[index]\n",
        "            \n",
        "            x = list(x)\n",
        "            x.append(1.0)\n",
        "            x = np.array(x)\n",
        "\n",
        "            derivatives = [self.cal_partial_derivative(x,y,j) for j in range(self.k)]\n",
        "            for j in range(self.k):\n",
        "                self.w[j] -= self.learning_rate * derivatives[j]  #梯度下降\n",
        "        return self.w\n",
        "        \n",
        "    \n",
        "        \n",
        "if __name__==\"__main__\":   \n",
        "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
        "        returnVec, classVec, test_size=0.3, random_state=2)\n",
        "    w = Softmax().train(train_features , list(train_labels))\n",
        "    print(\"w的维度：\" + str(w.shape))\n",
        "    print(\"test_features的维度：\" + str(np.array(test_features).shape))\n",
        "    print(\"test_labels的维度：\" + str(np.array(test_labels).shape))\n",
        "\n",
        "    \n",
        "#     print(list(train_labels))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w的维度：(5, 39)\n",
            "test_features的维度：(46818, 38)\n",
            "test_labels的维度：(46818,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu2pW3sFAcJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_(w,x):\n",
        "        result = np.dot(x,np.transpose(w))\n",
        "        row, column = result.shape\n",
        "        _positon = np.argmax(result)  # 找最大值所在的列\n",
        "        m, n = divmod(_positon, column)\n",
        "        return m\n",
        "\n",
        "def predict(features):\n",
        "    labels = []\n",
        "    for feature in features:\n",
        "        x = list(feature)\n",
        "        x.append(1)\n",
        "\n",
        "        x = np.matrix(x)\n",
        "        x = np.transpose(x)\n",
        "        labels.append(predict_(x))\n",
        "    return labels\n",
        "\n",
        "def feature_plus(feature):\n",
        "    feature_plus = []\n",
        "    for i in range(len(feature)):\n",
        "        feat = feature[i]\n",
        "        feat.append(1.0)\n",
        "        feature_plus.append(feat)\n",
        "    return feature_plus\n",
        "\n",
        "test_features_1 = feature_plus(test_features)\n",
        "M = [] \n",
        "for i in range(len(test_features_1)):\n",
        "    m = predict_(w,test_features_1)\n",
        "    M.append(m)\n",
        "return M\n",
        "print(M)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELBRfIuLnzf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}