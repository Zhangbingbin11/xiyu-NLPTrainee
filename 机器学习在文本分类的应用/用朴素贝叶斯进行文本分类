 1. 贝叶斯方法是一个历史悠久，有着坚实的理论基础的方法，同时处理很多问题时直接而又高效，很多高级自然语言
  处理模型也可以从它演化而来。因此，学习贝叶斯方法，是研究自然语言处理问题的一个非常好的切入口。

 2. 贝叶斯公式 + 条件独立假设 = 朴素贝叶斯方法
    贝叶斯公式：P(Y|X)=P(X|Y)P(Y)/P(X)
    条件独立假设：各个特征属性是条件独立的
  
 3. 用机器学习的视角理解贝叶斯公式
        在机器学习的视角下，我们把X理解成“具有某特征”，把Y理解成“类别标签”(一般机器学习问题中都是X=>特征, 
    Y=>结果)。在最简单的二分类问题(是与否判定)下，我们将Y理解成“属于某类”的标签。于是贝叶斯公式就变形
    成了下面的样子:
       
       P(“属于某类”|“具有某特征”)={P(“具有某特征”|“属于某类”)P(“属于某类”)}/{P(“具有某特征”)}
       
    而我们二分类问题的最终目的就是要判断P(“属于某类”∣“具有某特征”)是否大于1/2就够了。贝叶斯方法把计算
    “具有某特征的条件下属于某类”的概率转换成需要计算“属于某类的条件下具有某特征”的概率，而后者获取方法
    就简单多了，我们只需要找到一些包含已知特征标签的样本，即可进行训练。而样本的类别标签都是明确的，所以
    贝叶斯方法在机器学习里属于有监督学习方法。
    
 4. 垃圾邮件识别
        举个例子，我们现在要对邮件进行分类，识别垃圾邮件和普通邮件，如果我们选择使用朴素贝叶斯分类器，那
    目标就是判断P(“垃圾邮件”∣“具有某特征”)是否大于1/2。现在假设我们有垃圾邮件和正常邮件各1万封作为训练
    集。需要判断以下这个邮件是否属于垃圾邮件：
     
        邮件内容： “我司可办理正规发票（保真）17%增值税发票点数优惠！”
        
    也就是判断概率P(“垃圾邮件”∣“我司可办理正规发票（保真）17%增值税发票点数优惠！”)是否大于1/2。
    
 5. 分词
       一个现实的问题，垃圾邮件内容不可能和所给的一模一样，训练集是有限的，而句子的可能性则是无限的。
    所以覆盖所有句子可能性的训练集是不存在的。但是，句子的可能性无限，但是词语就那么些！所以就涉及到分
    词按人们的经验理解，两句话意思相近并不强求非得每个字、词语都一样。比如“我司可办理正规发票，17%增值
    税发票点数优惠！”，这句话就比之前那句话少了“（保真）”这个词，但是意思基本一样。如果把这些情况也考
    虑进来，那样本数量就会增加，这就方便我们计算了。
       于是，我们可以不拿句子作为特征，而是拿句子里面的词语（组合）作为特征去考虑。比如**“正规发票”可以
    作为一个单独的词语，“增值税”也可以作为一个单独的词语等等。(这里必然涉及到去停用词，去标点符号的处理)
        我们观察（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)，这可以理解
    成一个向量：向量的每一维度都表示着该特征词在文本中的特定位置存在。这种将特征拆分成更小的单元，依据这
    些更灵活、更细粒度的特征进行判断的思维方式，在自然语言处理与机器学习中都是非常常见又有效的。
        又根据条件独立性假设，因此贝叶斯公式就变成了：
                  P(Y|X)={P(X1|Y)P(X2|Y)P(X3|Y)~P(Xn|Y)P(Y)}/P(X)
    这个公式里面的每一项对应到垃圾邮件识别的例子中都是好求的。
    比如：P(X1|Y)-->P(“我”|垃圾邮件)=（垃圾邮件中所有“我”的次数）/（垃圾邮件中所有词语的次数）
    统计次数非常方便，而且样本数量足够大，算出来的概率比较接近真实。于是垃圾邮件识别的问题就可解了。
    
 6. 处理重复词语的三种方式
       我们之前的垃圾邮件向量（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,
   “优惠”)，其中每个词都不重复。而这在现实中其实很少见。因为如果文本长度增加，或者分词方法改变，必
   然会有许多词重复出现，因此需要对这种情况进行进一步探讨。
   三种模型：
   1.多项式模型：如果我们考虑重复词语的情况，也就是说，重复的词语我们视为其出现多次，直接
   按条件独立假设的方式推导。
   2.伯努利模型：将重复的词语都视为其只出现1次。
   3.混合模型：第三种方式是在计算句子概率时，不考虑重复词语出现的次数，但是在统计计算词语的概率
   P(“词语”|S）时，却考虑重复词语的出现次数，这样的模型可以叫作混合模型。
   实践中采用哪种模型，关键看具体的业务场景。笔者的简单经验是，对于垃圾邮件识别，混合模型更好些。
   
 7. 平滑技术
      比如：
   P(（“我”,“司”,“可”,“办理”,“正规发票”)|S)=P(“我”|S)P(“司”|S)P(“可”|S)P(“办理”|S)P(“正规发票”|S）
   
   然后我们扫描一下训练集，发现“正规发票”这个词从出现过！，于是P(“正规发票”∣S）=0...问题严重了，整个
   概率都变成0了.朴素贝叶斯方法面对一堆0，很凄惨地失效了…更残酷的是这种情况其实很常见，因为哪怕训练集
   再大，也可能有覆盖不到的词语。本质上还是样本数量太少，不满足大数定律，计算出来的概率失真。为了解决
   这样的问题，一种分析思路就是直接不考虑这样的词语，但这种方法就相当于默认给P(“正规发票”|S）赋值为1。
   其实效果不太好，大量的统计信息给浪费掉了。我们进一步分析，既然可以默认赋值为1，为什么不能默认赋值为
   一个很小的数？这就是平滑技术的基本思路。
        所有的平滑技术都是给未出现在训练集中的词语一个估计的概率，而相应地调低其他已经出现的词语的概率。
   平滑技术是因为数据集太小而产生的现实需求。如果数据集足够大，平滑技术对结果的影响将会变小。
   
 8. 小结
      我们找了个最简单常见的例子：垃圾邮件识别，说明了一下朴素贝叶斯进行文本分类的思路过程。基本思路是
   先区分好训练集与测试集，对文本集合进行分词、去除标点符号等特征预处理的操作，然后使用条件独立假设，将
   原概率转换成词概率乘积，再进行后续的处理。基于对重复词语在训练阶段与判断（测试）阶段的三种不同处理方
   式，我们相应的有伯努利模型、多项式模型和混合模型。在训练阶段，如果样本集合太小导致某些词语并未出现，
   我们可以采用平滑技术对其概率给一个估计值。而且并不是所有的词语都需要统计，我们可以按相应的“停用词”和
   “关键词”对模型进行进一步简化，提高训练和判断速度。
